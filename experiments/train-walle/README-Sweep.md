#  Sweep

This folder contains scripts to run hyperparameter sweeps using the Weights & Biases (W&B) platform.

<!-- insert a table of contents -->
- [ Sweep](#sweep)
  - [Ensure the model is functional](#ensure-the-model-is-functional)
  - [Sweep from Python script](#sweep-from-python-script)
  - [Sweep from command line](#sweep-from-command-line)
    - [sweep config example](#sweep-config-example)
    - [Sweep experiments](#sweep-experiments)
  - [ A note about validation and test batch size](#a-note-about-validation-and-test-batch-size)
    - [Understanding Graph Data Batching](#understanding-graph-data-batching)
    - [Node Classification in Graphs](#node-classification-in-graphs)
    - [Handling Batches for MCC Calculation](#handling-batches-for-mcc-calculation)

---

## Ensure the model is functional

Before running the sweep, ensure the model is functional by running the following tests:

```shell
```

## Sweep from Python script

To sweep using the Python script `sweep-stable.py`, you need to change the `sweep_config` dictionary in the script inside the function `run_sweep()`.

```python
# integrate with wandb sweep
def run_sweep():
    sweep_config = {
        "method": "bayes",  # or 'grid', 'random'
        "metric": {
            "name": "edge_index_bg_rec_loss",  # Define the metric for optimization
            "goal": "minimize",  # `minimize` 或 `maximize`
        },
        "parameters": {"hparams:train_batch_size": {"values": [64, 128]}},
    }
    # Initialize a sweep
    sweep_id = wandb.sweep(sweep_config, project=os.environ["WANDB_PROJECT"])
    wandb.agent(sweep_id, function=main)
```

then run it with the following command:

```shell
# with default hydra config defined in folder ./conf
python sweep-stable.py
# with custom hydra config e.g. under dev mode
python sweep-stable.py mode=dev
```

## Sweep from command line

Use the Python script `sweep-cli.py` to sweep from the command line. You can
change the sweep config defined in a YAML file e.g. `./sweep-test.yaml`.

```shell
wandb sweep sweep-example.yaml
# output
# wandb: Creating sweep from: sweep-test.yaml
# wandb: Creating sweep with ID: vsqjojes
# wandb: View sweep at: https://wandb.ai/WANDB_ENTITY/WANDB_PROJECT/sweeps/vsqjojes
# wandb: Run sweep agent with: wandb agent WANDB_ENTITY/WANDB_PROJECT/vsqjojes
```

- `vsqjojes` is the sweep id generated by wandb.
- `WANDB_ENTITY` and `WANDB_PROJECT` will show as your own entity and project name.

Then run the sweep agent with the following command:

```shell
wandb agent vsqjojes
```

### sweep config example

For details, please refer to wandb's documentations:
- [Sweep configuration structure](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration)
- [Initialize sweeps](https://docs.wandb.ai/guides/sweeps/initialize-sweeps)
- [Start sweep agents](https://docs.wandb.ai/guides/sweeps/start-sweep-agents)
- [Hydra with W&B](https://docs.wandb.ai/guides/integrations/hydra)
- [Use multiple GPUs for sweep](https://docs.wandb.ai/guides/sweeps/parallelize-agents)

We provide an example of the sweep config file `sweep-example.yaml` as in `sweep-example.yaml` with content:

```yaml
program: sweep.py
method: bayes
metric:
  name: edge_index_bg_rec_loss
  goal: minimize
parameters:
  hparams:
    parameters:
      train_batch_size:
        values: [128]

command:
  - ${env}
  - python
  - ${program}
  - 'mode=dev'
```
- `program` path to the sweep python script, accepts
  - absolute path
  - relative path to the current working directory (CWD)
- `method` search method, accepts
  - `grid`
  - `random`
  - `bayes`
- `metric`: the metric to optimize
  - `name`: the metric name
  - `goal`: the optimization goal, accepts
    - `minimize`
    - `maximize`
- `parameters`: the hyperparameters to sweep, it accepts nested parameters like the one in the example, for details refer to W&B documentation [Double nested parameters](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration#double-nested-parameters) and an example on this FAQ page [Nested Sweep Configuration](https://community.wandb.ai/t/nested-sweep-configuration/3369) ([credits: @mjvolk3](https://community.wandb.ai/u/mjvolk3) )
  - this example is sweeping the hyperparameter `hparams.train_batch_size` with values `[128]` i.e. this will always run with `train_batch_size=128` while the default value is `32`, it's good for debugging and purposes.

### Sweep experiments

Sweep YAML files are provided in the `./sweep-configs` folder. You can run the sweep with the following command:

```shell
wandb sweep sweep-configs/sweep-walle.yaml
# output
# wandb: Creating sweep from: sweep-configs/sweep-walle.yaml
# wandb: Creating sweep with ID: vsqjojes
# wandb: View sweep at: https://wandb.ai/WANDB_ENTITY/WANDB_PROJECT/sweeps/vsqjojes
# wandb: Run sweep agent with: wandb agent WANDB_ENTITY/WANDB_PROJECT/vsqjojes
```

Then run the sweep agent with the following command:

```shell
wandb agent vsqjojes

# to specify a specific GPU, e.g.
CUDA_VISIBLE_DEVICES=1 wandb agent vsqjojes
```

##  A note about validation and test batch size

### Understanding Graph Data Batching

*adapted from ChatGPT4 answer*

In PyTorch Geometric, a Data object typically represents a single graph or a part of a graph. When you use a DataLoader to create batches:

- **Single Large Graph**: If your dataset is a single large graph (like a social network or citation network), the entire graph is considered as one sample. Batching usually involves either dealing with the whole graph at once or using subgraphs but still, the node features and edge connections pertain to segments of the same overarching structure.
- **Multiple Graphs**: If your dataset consists of **multiple graphs** (like molecules in a chemical dataset), then each graph is a sample, and a batch consists of multiple such graphs. **PyTorch Geometric employs a dynamic batching process that merges multiple graphs into a single disjoint graph.** This disjoint graph has a combined adjacency matrix and node feature matrix where individual graphs don’t interact with each other unless specified (like in graph classification).

### Node Classification in Graphs

For node classification tasks within a single graph or multiple graphs:

Graph-Level MCC: If each graph is a sample, and you want to compute the MCC for each graph separately, you can calculate the MCC for each disjoint graph within a batch and then average these values.

Node-Level MCC: If your task is node classification within a single graph, and nodes are your "samples", calculating the MCC for individual nodes directly isn't straightforward because the DataLoader will return batches of nodes interconnected within the same large graph structure.

### Handling Batches for MCC Calculation

Given our use case where we want to compute the metrics e.g. MCC for individual samples (i.e. a pair of ab and ag graphs) and then average them:

Since we are dealing with multiple small graphs: we calculate the MCC for each graph within a batch. **This requires accumulating predictions and labels for each graph separately within the batch**, which can be managed using the batch tensor that PyTorch Geometric provides in its DataLoader output.

If dealing with a single graph with node classification: Here, the concept of sample-level MCC for each node separately is not directly applicable in a traditional sense because predictions are made in the context of the whole or sub-parts of the graph, not individual nodes isolated from their context.
